# ERNIE1.0-char

## character tokenizer

分词器按照token进行分词，vocab中仅包含长度为1的token（即英文也会按照字母逐个被切分）

## ernie1.0-char model checkpoint

模型中的word embedding也根据vocab的变化进行了相应的调整，
vocab 和模型checkpoint的下载地址：[ernie1.0-char](https://huggingface.co/peterchou/ernie1.0-char)
